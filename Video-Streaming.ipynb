{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Streaming\n",
    "\n",
    "Once video is captured and stored on a server, it can be streamed by a user by direct connection (the *client-server* model). Another model of service includes a middle-man of sorts: the video is first pushed to edge caches (servers placed at network edges), and the client loads the video from the closest of these servers (the *content-delivery network* model, used by e.g. Youtube). A *Peer-to-Peer* (P2P) network can also be established for video streaming. In a P2P network, users act as both clients and servers, by simulataneously downloading data from the network and uploading this data to other users. This method of downloading and streaming is highly efficient (leads to major bandwith savings for the distributor), but for streaming in particular this model may be quite sensitive to peer departures (although there are methods to combat this effect). \n",
    "\n",
    "Once connection to a server is established, there are a few common techniques for content delivery. The traditional web transfer protocal, *HTTP*, takes a request from a client and returns content in large segments. Imporvements built on this protocol have been developed for video streaming.  *DASH* is one modern video-streaming transfer protocol which segments the file, and transmits the content adaptively.\n",
    "\n",
    "The quality of a video stream -- measured subjectively -- is affected by the video capture, compression,  transmission, and also by factors on the audience end. We present a non-exhaustive tour of some important features.\n",
    "\n",
    "** Resolution : number of pixels displayed and/or captured. **   \n",
    "Commonly,\n",
    "- 720 x 480 (or 480p) \"SD\"\n",
    "- 1280 x 720 (or 720p)\"HD\"\n",
    "- 1920 x 1080 (or 1080p) \"Full HD\"\n",
    "- 2560 x 1440 (or 1440p) \"QHD\"\n",
    "- 3840 x 2160 (or 2160p) \"4K\"\n",
    "- 7680 x 4320 (or 4320p) \"8K\"\n",
    "\n",
    "** Framerate: rate at which frames are displayed. **\n",
    "Increasing the framerate (fps) will result in a smoother video at a given resolution. Standards for framerate vary from media to media. A typical computer screen has a framerate of 60fps. Movie theatres use a framerate of 24 fps.\n",
    "\n",
    "** Bitrate: Size of video file streaming per second. **\n",
    "\n",
    "Need to ensure that most user's have adequate bandwith. Trade-off between possible lags/buffering and loss of quality. \n",
    "Higher resolution not necessarily higher quality: need to take into account monitor of device on which the video will be streamed. Data may be captured at high resolution, but then sampled down for cost/efficiency. This is because higher resolution requires higher bitrate...Naturally, a higher framerate will also require higher bitrate.\n",
    "\n",
    "Higher motion and detail, for example, both require higher bitrate to maintain quality (note: most streams have variable bitrate for this reason). \n",
    "\n",
    "## Video Compression\n",
    "\n",
    "** Frame-by-Frame encoding ** \n",
    "Compress each individual frame, resulting in a series of compressed images. Since no information from other frames is required for compresison, the data can be compressed, transmitted, decompressed and then displayed rapidly (low latency). There is also no risk of invalid frames caused by the algorithm, which is useful for surveillance. The static-image compression algorithms used are generally JPEG or JPEG2000. \n",
    "\n",
    "** Temporal encoding **\n",
    "With temporal encoding, we also consider changes from frame to frame, attempting to only store the changes between frames. This can lead to more savings than frame-by-frame encoding, but runs a risk of missing information. Standard codecs (enocding/decoding algorithms) are H.264 and MPEG-4.\n",
    "\n",
    "In the original MPEG-4, we encode frames as a *Group of Pictures (GOP)* of three frame type: Intra(I)-frames, Predicted(P)-frames and Bidirectional(B)-frames. \n",
    "- I-Frames are complete encoded images (can be decoded without reference to other frames). Needed as starting points, reference points if stream is damaged, and for \"random access\" functions (e.g. fast-forwarding).  They contain no artifacts, but are, on the other hand, more expensive. \n",
    "- P-Frames are coded with reference to an earlier I- or P-frame. This reference is in general complex and, as such, they are sensitive to transmission errors. They require fewer bits than I-frames.\n",
    "- B-Frames contain information on the changes between previous and following frames. Lower latency can be achieved without using B-frames.\n",
    "\n",
    "This codec is used in low-resolution cameras.\n",
    "\n",
    "H.264 is an extension on MPEG-4. Additonal features include motion compensation (used in P- and B-frames): motion vectors are used to describe relative movement of an object from a reference frame. Computing the motion vector takes up fewer bits than if the whole content were to be coded (althout it is, in general, demanding). H.264 can also reduce the size of I-frames. Generally seen as the \"highest standard\" for video compression, on average, we can reduce bitrate by ~50% for fixed quality (less bandwith and storage space needed: higher quality at a lower bitrate!). High resolution cameras are needed, and results in higher latency.\n",
    "\n",
    "In our data set (provided by Roger Donaldson of Midvale Applied Mathematics), we are given data on various camera, scene and transmission data, but no user data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first look at the data\n",
    "\n",
    "Our goal will be to create a model which predicts bitrate given various camera and scene features.  Bitrate measurements were taken after capturing and compressing video under different test conditions (listed below). The data was provided by Roger Donaldson (Midvale Applied Mathematics).\n",
    "\n",
    "### Camera Features (from 8 different cameras)\n",
    "- Flicker: camera cuts out light flicker, depending on the region\n",
    "- KeyFrame: number of P- and B-frames between I-frames (minus 1)\n",
    "- ImageRate: framerate (fps)\n",
    "- Quality: compression control, low number is best quality\n",
    "- Nonlinear: single or multi-exposure frames (HDR)\n",
    "- Mode: high performance or standard modes\n",
    "- Compression: additional compression mode\n",
    "- KpbsLimit: user set maximum bitrate\n",
    "- Primary Resolution: highest resolution stream\n",
    "- Secondary Resolution: lower resolution stream\n",
    "- Tertiary Resolution: lowest resolution stream\n",
    "\n",
    "### Scene Features\n",
    "- Test: type of test (Base, Idle, Compression, or HDR)\n",
    "- Motion: amount of motion in scene, classified as high, medium or none\n",
    "- Detail: the amount of detial in scene, classified as high, medium, or low\n",
    "\n",
    "### Measurements\n",
    "- CollectSeconds: time of data collection\n",
    "- WaitSeconds: time between measurements\n",
    "- TotalBytes: total number of bytes transmitted to the network\n",
    "- Primary Bitrate: mean bitrate for primary stream\n",
    "- Secondary Bitrate: mean bitrate for secondary stream\n",
    "- Tertiary Bitrate: mean bitrate for tertiary stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# First load date from cameras\n",
    "A3data=pd.read_csv('../data/A3.csv')\n",
    "#D16data=pd.read_csv('../data/D16.csv')\n",
    "print(A3data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert resolution to number (log scale)\n",
    "def res_to_number(r):\n",
    "    s = np.zeros(r.shape[0])\n",
    "    for i in range(len(s)):\n",
    "        ri = r[i]\n",
    "        xloc = str.find(ri,'x')\n",
    "        s[i] = int(r[i][:xloc])*int(r[i][xloc+1:])\n",
    "    return s  \n",
    "\n",
    "PrimRes_num_A3 = np.log10(res_to_number(A3data['PrimaryResolution'].values))\n",
    "SecRes_num_A3 = np.log10(res_to_number(A3data['SecondaryResolution'].values))\n",
    "\n",
    "\n",
    "# Convert motion  and detail to number\n",
    "def categ_ordered_to_num(r,vals):\n",
    "    p = len(vals)\n",
    "    s = np.zeros(len(r))\n",
    "    for i in range(len(r)):\n",
    "        ri = r[i]\n",
    "        idx = vals.index(ri)\n",
    "        s[i] = idx\n",
    "    return s\n",
    "\n",
    "Detail_num_A3 = categ_ordered_to_num(A3data['Detail'].values,['low','medium','high'])\n",
    "Motion_num_A3 = categ_ordered_to_num(A3data['Motion'].values,['none','low','high'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect motion and resolution to be important factors. Look at the relationship between bitrate \n",
    "# and primary resolution for high,med,low motion\n",
    "Y = np.log10(A3data['PrimaryBitsPerSecond'].values)\n",
    "\n",
    "fig = plt.figure(figsize=(17,5))\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(PrimRes_num_A3,Y, c=Motion_num_A3, alpha=0.2)\n",
    "plt.colorbar()\n",
    "plt.ylabel('Primary Bitrate')\n",
    "plt.title('motion')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(PrimRes_num_A3,Y, c=Detail_num_A3, alpha=0.2)\n",
    "plt.colorbar()\n",
    "plt.ylabel('Primary Bitrate')\n",
    "plt.title('detail')\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(PrimRes_num_A3, Y, c=A3data['ImageRate'].values,alpha=0.2)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Primary Resolution')\n",
    "plt.ylabel('Primary Bitrate')\n",
    "plt.title('frame rate')\n",
    "\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(PrimRes_num_A3, Y, c=A3data['Quality'].values,alpha=0.2)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Primary Resolution')\n",
    "plt.ylabel('Primary Bitrate')\n",
    "plt.title('quality')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sources\n",
    "http://www.lcis.com.tw/paper_store/paper_store/DASH-IEEE-multimedia-preprint-201622602748584.pdf\n",
    "http://www4.comp.polyu.edu.hk/~oneprobe/doc/im2011-qoe.pdf\n",
    "http://www.itc23.com/fileadmin/ITC23_files/papers/a5.pdf\n",
    "http://ieeexplore.ieee.org.proxy.lib.sfu.ca/stamp/stamp.jsp?arnumber=7222404\n",
    "http://avigilon.com/assets/pdf/WhitePaperCompressionTechnologiesforHD.pdf\n",
    "https://www.axis.com/files/whitepaper/wp_h264_31669_en_0803_lo.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
