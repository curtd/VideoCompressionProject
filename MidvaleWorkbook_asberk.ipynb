{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/asberk/data/1-Donaldson/AllData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkColumn(df, colNum):\n",
    "    \"\"\"\n",
    "    Used in throwAwayUnchanged\n",
    "    \"\"\"\n",
    "    return np.all(df.iloc[0, colNum] == df.iloc[1:, colNum])\n",
    "\n",
    "\n",
    "def throwAwayUnchanged(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    Could, however, prove useful on subsetted-by-group data...\n",
    "    This function throws away columns that are the same in every entry\n",
    "    \"\"\"\n",
    "    idxUnhelpful = [j for j in range(df.columns.size)\n",
    "                    if checkColumn(df, j)]\n",
    "    df = df.drop(df.columns[idxUnhelpful], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def throwAwayBizarre(df):\n",
    "    \"\"\"\n",
    "    Throws away rows where TotalBytes is negative \n",
    "    (because this doesn't make sense).\n",
    "    \"\"\"\n",
    "    df = df.loc[df['TotalBytes'] >= 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def removeUnwanted(data):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    \"\"\"\n",
    "    # Don't worry about High Performance mode for this task\n",
    "    data = data.groupby('Mode').get_group(0)\n",
    "    # Flicker is not useful for prediction\n",
    "    data = data.drop('Flicker', axis=1)\n",
    "    # We will throw away columns that are all the same\n",
    "    # (On `data`, this gets rid of Sharpening, \n",
    "    #  WaitSeconds and Status)\n",
    "    data = throwAwayUnchanged(data)\n",
    "    data = throwAwayBizarre(data)\n",
    "    data = data.drop('Index', axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def fixMiscValues(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    \"\"\"\n",
    "    df = df.fillna({'TertiaryResolution': 'NaN'})\n",
    "    df = df.replace('-', value=0)\n",
    "    df['SecondaryBitsPerSecond'] = df['SecondaryBitsPerSecond'].astype(np.float64)\n",
    "    df['TertiaryBitsPerSecond'] = df['TertiaryBitsPerSecond'].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preProcess(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project\n",
    "    \"\"\"\n",
    "    df = removeUnwanted(df)\n",
    "    df = fixMiscValues(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preProcess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a subset of the data for a simpler time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to figure out the subset...\n",
    "\n",
    "Roger recommended sticking with `Test == Base` and a single camera. Let's choose the camera with the most observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_gb_CameraName = data.loc[data['Test']=='Base'].groupby(['CameraName'])\n",
    "CameraName_highestCountOf_Base = Base_gb_CameraName.count()['Test'].argmax()\n",
    "data_A3Base = Base_gb_CameraName.get_group(CameraName_highestCountOf_Base)\n",
    "data_A3Base = data_A3Base.drop(['CameraName', 'Test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_A3Base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logBytes = data.loc[:, ['TotalBytes', 'PrimaryBitsPerSecond', 'SecondaryBitsPerSecond', 'TertiaryBitsPerSecond']]\n",
    "logBytes = logBytes.replace(0., np.nan).apply(lambda x: np.log(x)).dropna(how='all')\n",
    "logBytes.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logTransformColumn(df, colname):\n",
    "    \"\"\"\n",
    "    Tailor-made for the Midvale data. \n",
    "    log-transforms the columns pertaining to bit-rate.\n",
    "    \"\"\"\n",
    "    logBytes = data[colname]\n",
    "    logBytes = logBytes.replace(0., np.nan).apply(lambda x: np.log(x))\n",
    "    logBytes = logBytes.dropna(how='all')\n",
    "    return df.assign(**{'log'+logBytes.name: logBytes})\n",
    "\n",
    "def logTransformBytes(df):\n",
    "    for columnName in ['TotalBytes', 'PrimaryBitsPerSecond', \n",
    "                       'SecondaryBitsPerSecond', 'TertiaryBitsPerSecond']:\n",
    "        df = logTransformColumn(df, columnName)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = logTransformBytes(data)\n",
    "data_A3Base = logTransformBytes(data_A3Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A3Base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_colVals(X, **kwargs):\n",
    "    \"\"\"\n",
    "    X : a categorical column of a data frame\n",
    "    \"\"\"\n",
    "    # Check if not categorical\n",
    "    #\n",
    "    #\n",
    "    # get value counts\n",
    "    vc = X.value_counts()\n",
    "    n = vc.shape[0]\n",
    "    xrange = np.arange(n)\n",
    "    plt.bar(xrange, vc.values, **kwargs)\n",
    "    plt.xticks(xrange, vc.index.tolist(), rotation=90)\n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of these\n",
    "# (these are the *useful* categories for CamerName:A3;Test:Base)\n",
    "categs = ['PrimaryResolution', 'SecondaryResolution', \n",
    "          'Keyframe', 'ImageRate', 'Quality',\n",
    "          'Detail', 'Motion']\n",
    "\n",
    "C = len(categs)\n",
    "ncols = 4\n",
    "nrows = np.int(np.ceil(C/5))\n",
    "figwidth = 20\n",
    "figheight = np.int(np.min([np.ceil(20/ncols*nrows), 20]))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(figwidth, figheight))\n",
    "fig.subplots_adjust(hspace=.75)\n",
    "\n",
    "for j, categ in enumerate(categs):\n",
    "    plt.subplot(nrows, ncols, j+1)\n",
    "    hist_colVals(data_A3Base[categ])\n",
    "    plt.xlabel(categ, labelpad=20)\n",
    "for j in range(C, nrows*ncols):\n",
    "    plt.subplot(nrows, ncols, j+1)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlations with PrimaryBitsPerSecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A3Base['logPrimaryBitsPerSecond'].hist(bins=1000, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "qualityResponse = scaler.fit_transform(data_A3Base.loc[:, ['Quality', 'logPrimaryBitsPerSecond']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist2d(qualityResponse[:,0],\n",
    "           qualityResponse[:,1],\n",
    "           bins=20);\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('log(PrimaryBitsPerSecond)')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_A3Base.loc[:,['Quality', 'ImageRate', 'Keyframe']].values,\n",
    "                                                    data_A3Base['logPrimaryBitsPerSecond'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ElasticNetCV(normalize=True)\n",
    "en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features for regression\n",
    "\n",
    "## Encoding the categoricals\n",
    "\n",
    "If there are any categoricals, then maybe we should put columns as integers so we can regress on them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setUpCategs(data, sparse=False):\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    lb = LabelEncoder()\n",
    "    oh = OneHotEncoder()\n",
    "    \n",
    "    categoricals = data.select_dtypes(include=['object'])\n",
    "    categoricals = pd.concat((categoricals, data['Nonlinear']), axis=1)\n",
    "    categoricals = categoricals.drop('Message', axis=1)\n",
    "    categoricals = categoricals.apply(lb.fit_transform)\n",
    "    categoricals = oh.fit_transform(categoricals)\n",
    "    if not sparse:\n",
    "        categoricals = categoricals.toarray()\n",
    "    return categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricals = setUpCategs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricals_A3Base = setUpCategs(data_A3Base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_names = ['Keyframe', 'ImageRate', 'Quality', 'KbpsLimit', 'CollectSeconds']\n",
    "numerical = data.filter(items=numerical_names)\n",
    "numerical_A3Base = data_A3Base.filter(items=numerical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical.values[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_A3Base.values[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the response(s)\n",
    "TotalBytes should never be negative so far as I'm aware, so let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_names = ['logTotalBytes', 'logPrimaryBitsPerSecond', 'logSecondaryBitsPerSecond', 'logTertiaryBitsPerSecond']\n",
    "responses = data.filter(items=response_names)\n",
    "responses_A3Base = data_A3Base.filter(items=response_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.values[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_A3Base.values[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First on a small problem, then on a progressively bigger ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A3Base = np.hstack((scaler.fit_transform(numerical_A3Base.values), categoricals_A3Base))\n",
    "y_A3Base = responses_A3Base.values\n",
    "y_A3Base_pbps = y_A3Base[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnanrows = [not np.isnan(y_A3Base_pbps[j]) for j in range(y_A3Base_pbps.shape[0])]\n",
    "\n",
    "X_nonnan = X_A3Base[nonnanrows, :]\n",
    "y_nonnan = y_A3Base_pbps[nonnanrows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There were no nan values to remove: {}'.format(X_A3Base.shape == X_nonnan.shape))\n",
    "print('Number of training examples is {}.'.format(X_nonnan.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating interaction features of degree ≤ 3...', end='')\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "X_nonnan_poly3 = poly.fit_transform(X_nonnan)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of poly features is {}.'.format(X_nonnan_poly3.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Splitting poly3 data into train and test...', end='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nonnan_poly3, y_nonnan)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsq = en.score(X_test, y_test)\n",
    "print('R^2 = {}'.format(Rsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! I'm getting an R^2 value of .932, which means I'm explaining about 93 % of the variance for the A3 Camera in the Base Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* We really want to predict `log(PrimaryBitsPerSecond)`. And we probably want to scale it first — Yes! Scaling the numerical sends the $R^2$ from .72 to .93!\n",
    "* What are the other variables we want to scale? (See above...)\n",
    "* Are we allowed to use Quality in our prediction? Yes! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
