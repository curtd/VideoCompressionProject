{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/asberk/data/1-Donaldson/AllData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkColumn(df, colNum):\n",
    "    \"\"\"\n",
    "    Used in throwAwayUnchanged\n",
    "    \"\"\"\n",
    "    return np.all(df.iloc[0, colNum] == df.iloc[1:, colNum])\n",
    "\n",
    "\n",
    "def throwAwayUnchanged(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    Could, however, prove useful on subsetted-by-group data...\n",
    "    This function throws away columns that are the same in every entry\n",
    "    \"\"\"\n",
    "    idxUnhelpful = [j for j in range(df.columns.size)\n",
    "                    if checkColumn(df, j)]\n",
    "    df = df.drop(df.columns[idxUnhelpful], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def throwAwayBizarre(df):\n",
    "    \"\"\"\n",
    "    Throws away rows where TotalBytes is negative \n",
    "    (because this doesn't make sense).\n",
    "    \"\"\"\n",
    "    df = df.loc[df['TotalBytes'] >= 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def removeUnwanted(data):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    \"\"\"\n",
    "    # Don't worry about High Performance mode for this task\n",
    "    data = data.groupby('Mode').get_group(0)\n",
    "    # Flicker is not useful for prediction\n",
    "    data = data.drop('Flicker', axis=1)\n",
    "    # We will throw away columns that are all the same\n",
    "    # (On `data`, this gets rid of Sharpening, \n",
    "    #  WaitSeconds and Status)\n",
    "    data = throwAwayUnchanged(data)\n",
    "    data = throwAwayBizarre(data)\n",
    "    data = data.drop('Index', axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def fixMiscValues(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    \"\"\"\n",
    "    df = df.fillna({'TertiaryResolution': 'NaN'})\n",
    "    df = df.replace('-', value=0)\n",
    "    df['SecondaryBitsPerSecond'] = df['SecondaryBitsPerSecond'].astype(np.float64)\n",
    "    df['TertiaryBitsPerSecond'] = df['TertiaryBitsPerSecond'].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preProcess(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project\n",
    "    \"\"\"\n",
    "    df = removeUnwanted(df)\n",
    "    df = fixMiscValues(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = preProcess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a subset of the data for a simpler time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to figure out the subset...\n",
    "\n",
    "Roger recommended sticking with `Test == Base` and a single camera. Let's choose the camera with the most observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Base_gb_CameraName = data.loc[data['Test']=='Base'].groupby(['CameraName'])\n",
    "CameraName_highestCountOf_Base = Base_gb_CameraName.count()['Test'].argmax()\n",
    "data_A3Base = Base_gb_CameraName.get_group(CameraName_highestCountOf_Base)\n",
    "data_A3Base = data_A3Base.drop(['CameraName', 'Test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_A3Base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logBytes = data.loc[:, ['TotalBytes', 'PrimaryBitsPerSecond', 'SecondaryBitsPerSecond', 'TertiaryBitsPerSecond']]\n",
    "logBytes = logBytes.replace(0., np.nan).apply(lambda x: np.log(x)).dropna(how='all')\n",
    "logBytes.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logTransformColumn(df, colname):\n",
    "    \"\"\"\n",
    "    Tailor-made for the Midvale data. \n",
    "    log-transforms the columns pertaining to bit-rate.\n",
    "    \"\"\"\n",
    "    logBytes = data[colname]\n",
    "    logBytes = logBytes.replace(0., np.nan).apply(lambda x: np.log(x))\n",
    "    logBytes = logBytes.dropna(how='all')\n",
    "    return df.assign(**{'log'+logBytes.name: logBytes})\n",
    "\n",
    "def logTransformBytes(df):\n",
    "    for columnName in ['TotalBytes', 'PrimaryBitsPerSecond', \n",
    "                       'SecondaryBitsPerSecond', 'TertiaryBitsPerSecond']:\n",
    "        df = logTransformColumn(df, columnName)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = logTransformBytes(data)\n",
    "data_A3Base = logTransformBytes(data_A3Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A3Base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_colVals(X, **kwargs):\n",
    "    \"\"\"\n",
    "    X : a categorical column of a data frame\n",
    "    \"\"\"\n",
    "    # Check if not categorical\n",
    "    #\n",
    "    #\n",
    "    # get value counts\n",
    "    vc = X.value_counts()\n",
    "    n = vc.shape[0]\n",
    "    xrange = np.arange(n)\n",
    "    plt.bar(xrange, vc.values, **kwargs)\n",
    "    plt.xticks(xrange, vc.index.tolist(), rotation=90)\n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of these\n",
    "# (these are the *useful* categories for CamerName:A3;Test:Base)\n",
    "categs = ['PrimaryResolution', 'SecondaryResolution', \n",
    "          'Keyframe', 'ImageRate', 'Quality',\n",
    "          'Detail', 'Motion']\n",
    "\n",
    "C = len(categs)\n",
    "ncols = 4\n",
    "nrows = np.int(np.ceil(C/5))\n",
    "figwidth = 20\n",
    "figheight = np.int(np.min([np.ceil(20/ncols*nrows), 20]))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(figwidth, figheight))\n",
    "fig.subplots_adjust(hspace=.75)\n",
    "\n",
    "for j, categ in enumerate(categs):\n",
    "    plt.subplot(nrows, ncols, j+1)\n",
    "    hist_colVals(data_A3Base[categ])\n",
    "    plt.xlabel(categ, labelpad=20)\n",
    "for j in range(C, nrows*ncols):\n",
    "    plt.subplot(nrows, ncols, j+1)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlations with PrimaryBitsPerSecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A3Base['logPrimaryBitsPerSecond'].hist(bins=1000, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "qualityResponse = scaler.fit_transform(data_A3Base.loc[:, ['Quality', 'logPrimaryBitsPerSecond']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist2d(qualityResponse[:,0],\n",
    "           qualityResponse[:,1],\n",
    "           bins=20);\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('log(PrimaryBitsPerSecond)')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_A3Base.loc[:,['Quality', 'ImageRate', 'Keyframe']].values,\n",
    "                                                    data_A3Base['logPrimaryBitsPerSecond'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ElasticNetCV(normalize=True)\n",
    "en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features for regression\n",
    "\n",
    "## Encoding the categoricals\n",
    "\n",
    "If there are any categoricals, then maybe we should put columns as integers so we can regress on them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categDF(data):\n",
    "    categoricals = data.select_dtypes(include=['object'])\n",
    "    categoricals = categoricals.drop('Message', axis=1)\n",
    "    return categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setUpCategs(data, sparse=False):\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    lb = LabelEncoder()\n",
    "    oh = OneHotEncoder()\n",
    "    categoricals = categDF(data)\n",
    "    categoricals = categoricals.apply(lb.fit_transform)\n",
    "    categoricals = oh.fit_transform(categoricals)\n",
    "    if not sparse:\n",
    "        categoricals = categoricals.toarray()\n",
    "    return categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unencodeOneHotLabelling(ohEnc, oh, lbl):\n",
    "    return lbl.inverse_transform(oh.active_features_)[np.argmax(ohEnc, axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricals = setUpCategs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricals_A3Base = setUpCategs(data_A3Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categ_df = categDF(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categ_df_A3Base = categDF(data_A3Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_df_A3Base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we unscramble categoricals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems = pd.DataFrame(np.array([['cat', 'dog', 'mouse', 'mouse', 'cat', 'guineapig'],['white', 'white', 'white', 'red', 'black', 'orange']]).T, columns=['animal', 'colour'])\n",
    "elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lblDemo = LabelEncoder()\n",
    "ohDemo = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lblDemo.fit_transform(elems.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lblDemo.inverse_transform([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohEncodingOfAnimals = ohDemo.fit_transform(np.c_[lblDemo.fit_transform(elems.values.ravel())]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lblDemo.inverse_transform(ohDemo.active_features_)[np.argmax(ohEncodingOfAnimals, axis=-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical_names = ['Keyframe', 'ImageRate', 'Quality', 'KbpsLimit', 'CollectSeconds']\n",
    "numerical = data.filter(items=numerical_names)\n",
    "numerical_A3Base = data_A3Base.filter(items=numerical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical.values[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_A3Base.values[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the response(s)\n",
    "TotalBytes should never be negative so far as I'm aware, so let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_names = ['logTotalBytes', 'logPrimaryBitsPerSecond', 'logSecondaryBitsPerSecond', 'logTertiaryBitsPerSecond']\n",
    "responses = data.filter(items=response_names)\n",
    "responses_A3Base = data_A3Base.filter(items=response_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.values[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_A3Base.values[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to unmuddle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCategNames(df):\n",
    "    return np.concatenate([np.unique(df[col].values) for col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unencodeOneHot(ohArr, df):\n",
    "    categNames = getCategNames(df)\n",
    "    return [categNames[np.where(ohArr)[1]][j::2] for j in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categNames_A3Base = getCategNames(categ_df_A3Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_A3Base = np.concatenate((numerical_names, categNames_A3Base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building regressors on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First on a small problem, then on a progressively bigger ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A3Base = np.hstack((scaler.fit_transform(numerical_A3Base.values), categoricals_A3Base))\n",
    "y_A3Base = responses_A3Base.values\n",
    "y_A3Base_pbps = y_A3Base[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonnanrows = [not np.isnan(y_A3Base_pbps[j]) for j in range(y_A3Base_pbps.shape[0])]\n",
    "\n",
    "X_nonnan = X_A3Base[nonnanrows, :]\n",
    "y_nonnan = y_A3Base_pbps[nonnanrows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Splitting vanilla data into train and test...', end='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nonnan, y_nonnan)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There were no nan values to remove: {}'.format(X_A3Base.shape == X_nonnan.shape))\n",
    "print('Number of training examples is {}.'.format(X_nonnan.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating interaction features of degree ≤ 2...', end='')\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_nonnan_poly2 = poly.fit_transform(X_nonnan)\n",
    "print('done.')\n",
    "\n",
    "print('Size of poly features is {}.'.format(X_nonnan_poly2.shape))\n",
    "\n",
    "print('Splitting poly2 data into train and test...', end='')\n",
    "Xp2_train, Xp2_test, yp2_train, yp2_test = train_test_split(X_nonnan_poly2, y_nonnan)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting on data with no interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsq_vanilla_EN = en.score(X_test, y_test)\n",
    "print('R^2 = {}'.format(Rsq_vanilla_EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_vanilla_EN = mean_squared_error(y_test, en.predict(X_test))\n",
    "print('MSE = {}'.format(MSE_vanilla_EN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting on data with interaction terms\n",
    "\n",
    "(**Note:** we do negligibly better with poly-degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.fit(Xp2_train, yp2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsq_poly2_EN = en.score(Xp2_test, yp2_test)\n",
    "print('R^2 = {}'.format(Rsq_poly2_EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_poly2_EN = mean_squared_error(yp2_test, en.predict(Xp2_test))\n",
    "print('MSE = {}'.format(MSE_poly2_EN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! I'm getting an R^2 value of .932, which means I'm explaining about 93 % of the variance for the A3 Camera in the Base Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* We really want to predict `log(PrimaryBitsPerSecond)`. And we probably want to scale it first — Yes! Scaling the numerical sends the $R^2$ from .72 to .93!\n",
    "* What are the other variables we want to scale? (See above...)\n",
    "* Are we allowed to use Quality in our prediction? Yes! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTree = ExtraTreesRegressor(n_jobs=-1)\n",
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "gb = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting RF on vanilla data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't fair..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_importance = np.argsort(gb.feature_importances_)[::-1]\n",
    "\n",
    "print(np.column_stack((names_A3Base, gb.feature_importances_))[idx_importance, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting RFs on data with degree 2 interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.fit(Xp2_train, yp2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.score(Xp2_test, yp2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(yp2_test, gb.predict(Xp2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super unfair..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polyFeatureNames_A3Base = poly.get_feature_names(names_A3Base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print those features that were **above $1\\sigma$ importance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_importance_sorted = gb.feature_importances_.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = gb.feature_importances_.mean() + gb.feature_importances_.std()\n",
    "for j in idx_importance_sorted:\n",
    "    coef = gb.feature_importances_[j]\n",
    "    if coef < th:\n",
    "        break\n",
    "    else:\n",
    "        ftrName = polyFeatureNames_A3Base[j]\n",
    "        coef = gb.feature_importances_[j]\n",
    "        print(ftrName, end='')\n",
    "        print(' '*(30-len(ftrName)), end='')\n",
    "        print(coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
