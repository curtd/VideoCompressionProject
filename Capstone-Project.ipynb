{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC Data Workshop - Video Compression Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = '/home/curtd/var/data/'\n",
    "datafile = 'AllData.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 50\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data by Aaron\n",
    "def checkColumn(df, colNum):\n",
    "    \"\"\"\n",
    "    Used in throwAwayUnchanged\n",
    "    \"\"\"\n",
    "    return np.all(df.iloc[0, colNum] == df.iloc[1:, colNum])\n",
    "\n",
    "def removeUnwantedCols(data):\n",
    "    drop_cols = ['Flicker','Index','Sharpening','TotalBytes','Status','Message',\n",
    "                 'TertiaryBitsPerSecond','TertiaryResolution']\n",
    "    for c in drop_cols:\n",
    "        data = data.drop(c,axis=1)\n",
    "    data.groupby('Mode').get_group(0)\n",
    "    data = removeUnchanging(data)\n",
    "    return data\n",
    "\n",
    "def removeUnchanging(data):\n",
    "    idxUnhelpful = [j for j in range(data.columns.size) if checkColumn(data,j)]\n",
    "    data = data.drop(data.columns[idxUnhelpful],axis=1)    \n",
    "    return data\n",
    "\n",
    "def fixMiscValues(data):\n",
    "    data = data.fillna({'TertiaryResolution' : 'NaN'})\n",
    "    data = data.replace('-', value=0)\n",
    "    data['SecondaryBitsPerSecond'] = data['SecondaryBitsPerSecond'].astype(np.float64)\n",
    "    #data['TertiaryBitsPerSecond'] = data['TertiaryBitsPerSecond'].astype(np.float64)\n",
    "    return data\n",
    "\n",
    "def appendFeatureName(data):\n",
    "    cols = ['Compression','Test','Detail','Motion','CameraName','PrimaryResolution','SecondaryResolution']\n",
    "    for c in cols:\n",
    "        data[c] = data[c].apply(lambda x: c + '_' + x)\n",
    "    return data\n",
    "\n",
    "def logTransformColumn(data, colname):\n",
    "    \"\"\"\n",
    "    Tailor-made for the Midvale data. \n",
    "    log-transforms the columns pertaining to bit-rate.\n",
    "    \"\"\"\n",
    "    logBytes = data[colname]\n",
    "    logBytes = logBytes.replace(0., np.nan).apply(lambda x: np.log10(x))\n",
    "    logBytes = logBytes.dropna(how='all')\n",
    "    return data.assign(**{'log'+logBytes.name: logBytes})\n",
    "\n",
    "def addLogCols(data):\n",
    "    for columnName in ['PrimaryBitsPerSecond', \n",
    "                       'SecondaryBitsPerSecond']:\n",
    "        data = logTransformColumn(data, columnName)\n",
    "    return data\n",
    "\n",
    "def preProcess(data):\n",
    "    data = removeUnwantedCols(data)\n",
    "    data = fixMiscValues(data)\n",
    "    data = addLogCols(data)\n",
    "    data = appendFeatureName(data)\n",
    "    return data\n",
    "\n",
    "# Read in data\n",
    "data = pd.read_csv(datadir + datafile)\n",
    "data = preProcess(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_names = ['Keyframe', 'ImageRate', 'Quality', 'KbpsLimit']\n",
    "numerical = data.filter(items=numerical_names)\n",
    "\n",
    "categ_names = ['PrimaryResolution', 'SecondaryResolution', 'Detail', 'Test', \n",
    "               'Motion','CameraName','Nonlinear','Mode','Compression']\n",
    "categ = data.filter(items=categ_names)\n",
    "\n",
    "response_names = ['logPrimaryBitsPerSecond', 'logSecondaryBitsPerSecond']\n",
    "responses = data.filter(items=response_names)\n",
    "\n",
    "excluded_names = [j for j in data.columns if (j not in categ_names and j not in numerical_names and j not in response_names and j not in ['PrimaryBitsPerSecond','SecondaryBitsPerSecond']) ]\n",
    "print('Included numerical features',numerical_names)\n",
    "print('Included categorical features',categ_names)\n",
    "print('Excluded features',excluded_names)\n",
    "\n",
    "# Data processing by Aaron\n",
    "def categDF(data):\n",
    "    categoricals = data.select_dtypes(include=['object'])\n",
    "    #categoricals = categoricals.drop('Message', axis=1)\n",
    "    return categoricals\n",
    "\n",
    "def setUpCategs(data, sparse=False):\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    lb = LabelEncoder()\n",
    "    oh = OneHotEncoder()\n",
    "    categoricals = categDF(data)\n",
    "    categoricals = categoricals.apply(lb.fit_transform)\n",
    "    categoricals = oh.fit_transform(categoricals)\n",
    "    if not sparse:\n",
    "        categoricals = categoricals.toarray()\n",
    "    return categoricals\n",
    "\n",
    "def unencodeOneHotLabelling(ohEnc, oh, lbl):\n",
    "    return lbl.inverse_transform(oh.active_features_)[np.argmax(ohEnc, axis=-1)]\n",
    "\n",
    "categoricals = setUpCategs(categ)\n",
    "categ_data = categDF(categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def getCategNames(df):\n",
    "    return np.concatenate([np.unique(df[col].values) for col in df.columns])\n",
    "\n",
    "def unencodeOneHot(ohArr, df):\n",
    "    categNames = getCategNames(df)\n",
    "    return [categNames[np.where(ohArr)[1]][j::2] for j in range(2)]\n",
    "\n",
    "categ_names = getCategNames(categ_data)\n",
    "data_names = np.concatenate((numerical_names, categ_names))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = np.hstack((scaler.fit_transform(numerical.values), categoricals))\n",
    "y = responses.values\n",
    "y_pbps = y[:, 0]\n",
    "nonnanrows = [not np.isnan(y_pbps[j]) for j in range(y_pbps.shape[0])]\n",
    "X_nonnan = X[nonnanrows, :]\n",
    "y_nonnan = y_pbps[nonnanrows]\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X_nonnan,y_nonnan)\n",
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "nfeat = X_train.shape[1]\n",
    "print('# of training samples {}'.format(ntrain))\n",
    "print('# of test samples {}'.format(ntest))\n",
    "print('# of features {}'.format(nfeat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output statistics for after regression\n",
    "res_func_inv = lambda x: 10**x\n",
    "pdeviation = 0.2\n",
    "def RMSE(output, prediction):\n",
    "    return np.sqrt(np.mean((res_func_inv(output) - res_func_inv(prediction))**2))\n",
    "def NRMSE(output, prediction):\n",
    "    return RMSE(output,prediction)/(np.max(res_func_inv(output))-np.min(res_func_inv(output)))\n",
    "def percentHits(output,prediction,pdeviation):\n",
    "    return np.mean( np.abs(res_func_inv(output)-res_func_inv(prediction))<pdeviation*abs(res_func_inv(output)) )\n",
    "\n",
    "def regressionSummary(method,Xtrain,Ytrain,Xtest,Ytest):\n",
    "    predict_train = method.predict(Xtrain)\n",
    "    train_nrmse = NRMSE(Ytrain,predict_train)\n",
    "    train_hits = percentHits(Ytrain,predict_train,pdeviation)\n",
    "    Rsq_train = method.score(Xtrain,Ytrain)\n",
    "    predict_test = method.predict(Xtest)    \n",
    "    test_nrmse = NRMSE(Ytest,predict_test)\n",
    "    test_hits = percentHits(Ytest,predict_test,pdeviation)\n",
    "    Rsq_test = method.score(Xtest,Ytest)\n",
    "    print('Regression results for ',method.__class__)\n",
    "    print('------Training data------')\n",
    "    print('NRMSE = {}'.format(train_nrmse))\n",
    "    print('Percentage of values with <',pdeviation*100,'percent rel err =',train_hits)\n",
    "    print('R^2 = {}'.format(Rsq_train))\n",
    "    print('------Test data----------')\n",
    "    print('NRMSE = {}'.format(test_nrmse))\n",
    "    print('Percentage of values with <',pdeviation*100,'percent rel err =',test_hits)\n",
    "    print('R^2 = {}'.format(Rsq_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "regressionSummary(lr,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plain elastic net\n",
    "if ntrain < 5000:\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "    en = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=-1)\n",
    "    en.fit(X_train,y_train)\n",
    "    regressionSummary(en,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ntrain < 5000:\n",
    "    # Kernel ridge regression \n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "    kr = KernelRidge(alpha=1,kernel='rbf',gamma=0.1)\n",
    "    kr.fit(X_train,y_train)\n",
    "    regressionSummary(kr,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=10)\n",
    "rf.fit(X_train,y_train)\n",
    "regressionSummary(rf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "et = ExtraTreesRegressor()\n",
    "et.fit(X_train,y_train)\n",
    "regressionSummary(et,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting - takes a while\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor(n_estimators=100)\n",
    "gb.fit(X_train,y_train)\n",
    "regressionSummary(gb,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances for Extra Trees Regressor\n",
    "importances = et.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in et.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "dispindex = np.where(importances[indices] < 0.05*importances[indices][0])[0][0]\n",
    "importances_trunc = importances[indices][:dispindex]\n",
    "names_trunc = data_names[indices][:dispindex]\n",
    "std_trunc = std[indices][:dispindex]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(dispindex),importances_trunc,color=\"r\",yerr=std_trunc)\n",
    "plt.xticks(range(dispindex),names_trunc,rotation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
